<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" /><updated>2022-07-12T21:41:38-06:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Research Notebook</title><subtitle>My personal blog.</subtitle><author><name>Ibrohim Nosirov</name></author><entry><title type="html">SVD, PCA, and SVM</title><link href="http://localhost:4000/SVD-PCA-and-SVM/" rel="alternate" type="text/html" title="SVD, PCA, and SVM" /><published>2022-07-12T00:00:00-06:00</published><updated>2022-07-12T00:00:00-06:00</updated><id>http://localhost:4000/SVD-PCA-and-SVM</id><content type="html" xml:base="http://localhost:4000/SVD-PCA-and-SVM/">&lt;p&gt;These are some rough notes I am writing for personal use/later revision ONLY.
None of this information is in any way verified, or correct… probably.&lt;/p&gt;

&lt;h2 id=&quot;singular-value-decomposition&quot;&gt;Singular Value Decomposition&lt;/h2&gt;
&lt;p&gt;This is a good way of representing a matrix that gives us insight into the
range, null space, rank, and 2-norm condition number. Geometrically, the SVD
posits that any matrix can be broken down into a rotation, a stretch, and a
rotation, in that order. Furthermore, for any matrix \(A\), writing out
\(A^\top A\) and \(AA^\top\) in terms of the SVD of \(A\) yields the
eigendecomposition of \(A^\top A\) and \(AA^\top\), respectively. Hence, the
eigenvectors of \(A^\top A\) and \(AA^\top\) make up the \(U\) and \(V\)
vectors, respectively.&lt;/p&gt;

&lt;p&gt;Also, taking the first column of \(U\), the first eigenvalue in \(\Sigma\), and
the first row of \(V^\top\) gives us a rank 1 matrix (column rank and row rank
are both 1, since they must be equal) with the greatest variance.&lt;/p&gt;

&lt;h2 id=&quot;principal-components-analysis&quot;&gt;Principal Components Analysis&lt;/h2&gt;
&lt;p&gt;Supposing we have a set of vectors, the PCA algorithm follows:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Find mean vector.&lt;/li&gt;
  &lt;li&gt;Create a mean-adjusted (subtract mean vector from every data vector) matrix,
\(S_{mean}\).&lt;/li&gt;
  &lt;li&gt;Compute a covariance matrix, \(Cov_{i,j} = (S_i-\bar{S})(S_j-\bar{S})\)
where \(\bar{S}\) is the mean vector and \(S\) is some vector in the set.&lt;/li&gt;
  &lt;li&gt;Compute the SVD of the covariance matrix. The rows of \(V^\top\) are the
principal axes and the columns of \(U\Sigma\) contain the principal scores.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;support-vector-machine-linear&quot;&gt;Support Vector Machine (Linear)&lt;/h2&gt;
&lt;p&gt;Support vectors are points in the data that, if removed, would change the
hyperplane. We use these support vectors in a constrained optimization problem
to define a separating hyperplane. In fact, we first define to two hyperplanes
(aka. the margins) such that:&lt;/p&gt;

&lt;p&gt;\(wx_i+b \geqslant 1\) when \(y_i = 1,\)&lt;/p&gt;

&lt;p&gt;\(wx_i+b \geqslant -1\) when \(y_i = -1.\)&lt;/p&gt;

&lt;p&gt;A key feature of these margins, which we incorporate more explicitly below, is
that no points can exist within the the boundaries of the margins. It follows
that the median of these hyperplanes is the separating hyperplane with the
maximum distance between any two members of different groups.&lt;/p&gt;

&lt;p&gt;This is done using a constrained optimization problem. It is defined as
follows:&lt;/p&gt;

&lt;p&gt;\(\min f: \frac{1}{2} \|w\|^2\) s.t. \(g: y_i(wx_i)-b=1.\)&lt;/p&gt;

&lt;p&gt;We solve this using the Lagrangian Dual Problem.&lt;/p&gt;

&lt;h2 id=&quot;support-vector-machine-non-linear&quot;&gt;Support Vector Machine (Non-Linear)&lt;/h2&gt;
&lt;p&gt;Realize that the dual problem involves computing inner products and that
kernels are, by definition, inner products in a reproducing kernel Hilbert
space. Hence, we just compute the feature map of each vector and compute inner
products just as in the linear case.&lt;/p&gt;</content><author><name>Ibrohim Nosirov</name></author><summary type="html">These are some rough notes I am writing for personal use/later revision ONLY. None of this information is in any way verified, or correct… probably. Singular Value Decomposition This is a good way of representing a matrix that gives us insight into the range, null space, rank, and 2-norm condition number. Geometrically, the SVD posits that any matrix can be broken down into a rotation, a stretch, and a rotation, in that order. Furthermore, for any matrix \(A\), writing out \(A^\top A\) and \(AA^\top\) in terms of the SVD of \(A\) yields the eigendecomposition of \(A^\top A\) and \(AA^\top\), respectively. Hence, the eigenvectors of \(A^\top A\) and \(AA^\top\) make up the \(U\) and \(V\) vectors, respectively. Also, taking the first column of \(U\), the first eigenvalue in \(\Sigma\), and the first row of \(V^\top\) gives us a rank 1 matrix (column rank and row rank are both 1, since they must be equal) with the greatest variance. Principal Components Analysis Supposing we have a set of vectors, the PCA algorithm follows: Find mean vector. Create a mean-adjusted (subtract mean vector from every data vector) matrix, \(S_{mean}\). Compute a covariance matrix, \(Cov_{i,j} = (S_i-\bar{S})(S_j-\bar{S})\) where \(\bar{S}\) is the mean vector and \(S\) is some vector in the set. Compute the SVD of the covariance matrix. The rows of \(V^\top\) are the principal axes and the columns of \(U\Sigma\) contain the principal scores. Support Vector Machine (Linear) Support vectors are points in the data that, if removed, would change the hyperplane. We use these support vectors in a constrained optimization problem to define a separating hyperplane. In fact, we first define to two hyperplanes (aka. the margins) such that: \(wx_i+b \geqslant 1\) when \(y_i = 1,\) \(wx_i+b \geqslant -1\) when \(y_i = -1.\) A key feature of these margins, which we incorporate more explicitly below, is that no points can exist within the the boundaries of the margins. It follows that the median of these hyperplanes is the separating hyperplane with the maximum distance between any two members of different groups. This is done using a constrained optimization problem. It is defined as follows: \(\min f: \frac{1}{2} \|w\|^2\) s.t. \(g: y_i(wx_i)-b=1.\) We solve this using the Lagrangian Dual Problem. Support Vector Machine (Non-Linear) Realize that the dual problem involves computing inner products and that kernels are, by definition, inner products in a reproducing kernel Hilbert space. Hence, we just compute the feature map of each vector and compute inner products just as in the linear case.</summary></entry><entry><title type="html">Fast Kernel Transform</title><link href="http://localhost:4000/Fast-Kernel-Transform/" rel="alternate" type="text/html" title="Fast Kernel Transform" /><published>2022-06-30T00:00:00-06:00</published><updated>2022-06-30T00:00:00-06:00</updated><id>http://localhost:4000/Fast-Kernel-Transform</id><content type="html" xml:base="http://localhost:4000/Fast-Kernel-Transform/"></content><author><name>Ibrohim Nosirov</name></author><summary type="html"></summary></entry><entry><title type="html">Hutch++: Algorithm for Trace Estimation</title><link href="http://localhost:4000/Hutch++/" rel="alternate" type="text/html" title="Hutch++: Algorithm for Trace Estimation" /><published>2022-06-10T00:00:00-06:00</published><updated>2022-06-10T00:00:00-06:00</updated><id>http://localhost:4000/Hutch++</id><content type="html" xml:base="http://localhost:4000/Hutch++/">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;</content><author><name>Ibrohim Nosirov</name></author><summary type="html">Introduction</summary></entry></feed>